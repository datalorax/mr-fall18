---
title: "Interactions"
author: "Daniel Anderson"
date: "Week 7"
output:
  xaringan::moon_reader:
    css: ["default", "uo", "uo-fonts", "hygge", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: atelier-dune-light
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "https://platform.twitter.com/widgets.js"
---

```{r setup, include = FALSE}
library(here)
library(tidyverse)
library(rio)

theme_set(theme_minimal(base_size = 25))

knitr::opts_chunk$set(fig.width = 13, 
                      message = FALSE, 
                      warning = FALSE)
options(pillar.sigfig = 7,
        scipen = 999)
```
# Agenda 
* Review Exercise
* Centering data
* Standardized coefficients (briefly)
* Interactions (this week and next)
  + Interactions between two categorical variables
  + Interactions between one categorical variable and one continuous variable
  + Interactions between two continuous variables
* Lab

---
class: inverse middle center
background-image:url(../img/chalkboard.jpg)
background-size:cover

# What questions do you have?


---
class: inverse middle
background-image:url(../img/wood.jpg)
background-size:cover

# Let's get setup

### Tonight, we'll work with multiple data sources
* diagnostics.sav
* benchmarks.xlsx
* hsb2.sav
* glo_sim.sav


---
class: inverse right
background-image:url(../img/bullseye.jpg)
background-size:cover

# Centering

---
# What the...ðŸ¤”ðŸ¤¨ðŸ¤¯

What does the intercept represent here?

.Large[
$$
weight\_{yc} = b\_0 + b1(weight\_{m}) + e\_i
$$
]

Where
* $weight_{yc}$ represents the weight of the youngest child, provided by the 
  variable `ycweight`.
* $weight_{m}$ represents the weight of the mother, provided by the variable
  `mweight`.

---
# Fit the model

```{r m1}
diag <- import(here("data", "diagnostics.sav"),
            setclass = "tbl_df") %>%
      characterize()
      
m1 <- lm(ycweight ~ mweight, diag)
arm::display(m1, detail = TRUE)  
confint(m1)
```  

---
# What does this look like?

```{r scatter1, fig.height = 6}
ggplot(diag, aes(mweight, ycweight)) +
  geom_point() +
  geom_smooth(method = "lm")
```

---
# Backwards projection

```{r scatter2, fig.height = 5.5}
ggplot(diag, aes(mweight, ycweight)) +
  geom_point() +
  geom_smooth(method = "lm",
              fullrange = TRUE) +
  xlim(0, 245)
```

---
```{r scatter3, fig.height = 4.5}
ggplot(diag, aes(mweight, ycweight)) +
  geom_point() +
  geom_smooth(method = "lm",
              fullrange = TRUE) +
  xlim(0, 245) +
  geom_point(data = data.frame(mweight = 0,
                               ycweight = coef(m1)[1]),
             size = 7,
             color = "magenta")
```

---
# Centering
Basically, we want to move the y-axis so it represents a meaningful value

--
### How?
Subtract a constant from each case


--
How about... 200! Why? ðŸ¤·

--
```{r mutate}
diag <- diag %>%
  mutate(weight200 = mweight - 200)
```

---
# Let's look at scatterplots

```{r scatters1a, fig.height = 6}
ggplot(diag, aes(mweight, ycweight)) +
  geom_point()
```

---
# Let's look at scatterplots

```{r scatters1b, fig.height = 6}
ggplot(diag, aes(weight200, ycweight)) +
  geom_point()
```

---
# Overlay linear regression line

```{r scatters2a, fig.height = 6}
ggplot(diag, aes(mweight, ycweight)) +
  geom_point() +
  geom_smooth(method = "lm")
```

---
# Overlay linear regression line

```{r scatters2b, fig.height = 6}
ggplot(diag, aes(weight200, ycweight)) +
  geom_point() +
  geom_smooth(method = "lm", color = "magenta")

```

---
class: inverse center middle
# What is the centering impacting?


---
# Fit model

```{r mod-centered}
m1c_200 <- lm(ycweight ~ weight200, diag)
arm::display(m1c_200, detail = TRUE)
```

--

```{r mod1-summary}
arm::display(m1, detail = TRUE)
```


---
# Look back at our plots


```{r scatter-notcentered-ref, echo = FALSE}
ggplot(diag, aes(mweight, ycweight)) +
  geom_point() +
  geom_smooth(method = "lm",
              fullrange = TRUE) +
  geom_point(data = data.frame(mweight = 0,
                  ycweight = coef(m1)[1]),
             size = 6, 
             color = "magenta") +
  geom_segment(x = 0, xend = 0,
               y = -Inf, yend = coef(m1)[1],
              color = "magenta",
              linetype = "dashed",
              size = 1.5) +
  geom_segment(x = -Inf, xend = 0,
               y = coef(m1)[1], yend = coef(m1)[1],
              color = "magenta",
              linetype = "dashed",
              size = 1.5) +
  annotate("text", 
           x = 20, 
           y = -10, 
           size = 8,
           label = "b[0]==7.84", 
           parse = TRUE,
           color = "magenta")

```

---
# Look back at our plots

```{r scatter-centered-ref, echo = FALSE}
ggplot(diag, aes(weight200, ycweight)) +
  geom_point() +
  geom_smooth(method = "lm",
              color = "magenta") +
  geom_point(data = data.frame(weight200 = 0,
                  ycweight = coef(m1c_200)[1]),
             size = 6, 
             color = "cornflowerblue") +
  geom_segment(x = 0, xend = 0,
               y = -Inf, yend = coef(m1c_200)[1],
              color = "cornflowerblue",
              linetype = "dashed",
              size = 1.5) +
  geom_segment(x = -Inf, xend = 0,
               y = coef(m1c_200)[1], yend = coef(m1c_200)[1],
              color = "cornflowerblue",
              linetype = "dashed",
              size = 1.5) +
  annotate("text", 
           x = -105, 
           y = 120, 
           size = 8,
           label = "b[0]==110.44", 
           parse = TRUE,
           color = "cornflowerblue")

```

---
# More common
* Center according to the mean

--
* Discuss in your tables, what would the intercept represent in the following
model

```{r centerd-exercise}
diag <- diag %>%
  mutate(age_c = ycage - mean(ycage),
         weight_c = mweight - mean(mweight))

m2 <- lm(ycweight ~ age_c + weight_c, diag)
```

---
```{r display-m2}
arm::display(m2, detail = TRUE)
```

--
### Example writeup for the intercept
Both age and weight were grand-mean centered, leading the intercept to 
represent the expected weight of the youngest child who were 10.54 years old 
and had birth mothers who weighed 154.71 pounds (both representing the sample
means).

---
class: inverse center middle
# Standardized coefficients

---
# Standardization

### Pop quiz
What does it mean to standardize a variable?

--
* Transform the variable to have a mean of 0 (same as grand-mean centering)
**and has a standard deviation of 1.0**. 


--
### How?
Grand-mean center, then divide each observation by its standard deviation.

---

```{r scipen-on, include = FALSE}
options(scipen = 999)
```

```{r standardize1}
diag <- diag %>%
  mutate(mweight_z = (mweight - mean(mweight)) / sd(mweight))

mean(diag$mweight_z)
sd(diag$mweight_z)
```

```{r scipen-off, include = FALSE}
options(scipen = 0)
```

--
### Pop Quiz

What would the intercept represent in this model? What about the slope for
weight?

```{r m1z}
m1z_a <- lm(ycweight ~ mweight_z, diag)
```

---
```{r display-m1z_a}
arm::display(m1z_a)

```

* **Intercept**: Same as the centered version - expected weight of the youngest
child when the birth mother has the average sample weight.

* **Slope**: Expected change in youngest child weight **given a one standard
deviation increase in birth mother weight**.


--
* Highly interpretable (imo), but almost never used. Why? ðŸ¤·â€â™€ï¸

--
* **NOT** standardized coefficient

---
# Standardized coefficient
* To get a standardized coefficient we have to standardize both the x and y
variable.
* Easier way to standardize is to use the `scale` function

```{r standardized-model}
diag <- diag %>%
  mutate(mweight_z = scale(mweight),
         ycweight_z = scale(ycweight))
```

--
### Pop Quiz
What would the intercept/slope represent now?

```{r m1z_b}
m1z_b <- lm(ycweight_z ~ mweight_z, diag)
```

---
```{r display-m1z_b}
arm::display(m1z_b, detail = TRUE)
```

--
* In simple linear regression with standardized coefficients: 
  + The coefficient is equal to $r$

--

```{r cor}
cor(diag$mweight, diag$ycweight)
```

--
* For all standardized coefficients:
  + The intercept will always be zero 
  + Coefficients (slopes) represent the expected SD change in
    the outcome, given a 1 SD increase in the corresponding
    predictor, holding all other variables constant



---
# Want a simpler solution?

```{r lmbeta1}
# install.packages("lm.beta")
library(lm.beta)
lm.beta(m1) 
```

---
Note that this works with arbitrarily complex models

```{r lmbeta2}
m_complex <- lm(ycweight ~ mage + mheight + mweight + 
                           fage + fheight + fweight +
                           ycage + ycheight, 
                           data = diag)
lm.beta(m_complex)
```

---
class: inverse center middle
# Interactions

--
### One categorical IV, one continuous IV

---
# Moderating effects
* Every model we've fit so far has assumed parallel slopes. That will now
change!

--

```{r interaction-plot1-echo, eval = FALSE}
benchmarks <- import(here("data", "benchmarks.xlsx"),
                     setclass = "tbl_df")

ggplot(benchmarks, aes(rdg_fall, rdg_spr)) +
  geom_point(color = "gray70") +
  geom_smooth(aes(color = ell),
              method = "lm")
```

---
```{r interaction-plot1-eval, echo = FALSE, fig.height = 9}
benchmarks <- import(here("data", "benchmarks.xlsx"),
                     setclass = "tbl_df")

ggplot(benchmarks, aes(rdg_fall, rdg_spr)) +
  geom_point(color = "gray70") +
  geom_smooth(aes(color = ell),
              method = "lm")
```

---
Prior plot shows pretty clear evidence that the slopes are not the same
across English Language learner classifications.


--
To this point, we'd model this as

```{r parallel}
benchmarks <- benchmarks %>%
  mutate(ell = factor(ell),
         ell = relevel(ell, ref = "Non-ELL"))

m3a <- lm(rdg_spr ~ rdg_fall + ell, benchmarks)
arm::display(m3a, detail = TRUE)
```

---
What does this model look like?
### One look

```{r show-prallel1, echo = FALSE}
benchmarks <- benchmarks %>%
  mutate(m3a_fitted = fitted(m3a))

ggplot(benchmarks, aes(rdg_fall, rdg_spr)) +
  geom_point(color = "gray80") +
  geom_line(aes(color = ell, y = m3a_fitted)) 
```

---
### Another look
```{r show-prallel2, echo = FALSE}
ggplot(benchmarks, aes(rdg_fall, rdg_spr)) +
  geom_point(data = benchmarks[ ,-5],
             color = "gray80") +
  geom_point(color = "magenta") +
  geom_line(aes(group = ell, y = m3a_fitted),
            color = "magenta") +
  facet_wrap(~ell)
```

---
```{r overlaid, echo = FALSE}
ggplot(benchmarks, aes(rdg_fall, rdg_spr)) +
  geom_point(data = benchmarks[ ,-5],
             color = "gray80") +
  geom_point(aes(color = ell)) +
  geom_point(aes(color = ell)) +
  geom_line(aes(group = ell, y = m3a_fitted),
            color = "magenta",
            size = 1.3) +
  geom_smooth(aes(color = ell),
              method = "lm",
              se = FALSE) +
  facet_wrap(~ell) +
  guides(color = "none")
```

The above plot shows the difference between our parallel slopes model (pink
lines), and a model where the slope is allowed to vary by group (colored 
lines).


---
## Thinking about modeling the interaction
* Interactions imply that the impact or effect of one variable **depends upon**
the level of another variable


--
* We can think about this from the perspective of separate models

---
* First, we specify our parallel slopes model

$$
Rdg\_{spr\_i} = b\_0 + b\_1(Rdg\_{fall\_i}) + b\_2(ELL\_i) + e
$$

--
* Next, we specify that the fall slope will be modeled by---is conditional 
upon---ELL status

$$
b\_1 = \gamma\_0 + \gamma\_1(ELL)
$$

--
* Substitute in and we get

$$
Rdg\_{spr\_i} = b\_0 + (\gamma\_0 + \gamma\_1(ELL))(Rdg\_{fall\_i}) + b\_2(ELL\_i) + e
$$

--
* Evaluate 

$$
Rdg\_{spr\_i} = b\_0 + \gamma\_0(Rdg\_{fall\_i}) + \gamma\_1(ELL \times Rdg\_{fall\_i}) + b\_2(ELL\_i) + e
$$

--
* Rearrange, and swap out the gammas for b's

$$
Rdg\_{spr\_i} = b\_0 + b\_1(Rdg\_{fall\_i}) + b\_2(ELL\_i) + b\_3(ELL \times Rdg\_{fall\_i})  + e
$$


---
# Take home message
We specify interactions by multiplying the terms

--
### Why?


--
When you multiply the terms, you are implicitly modeling the effect of one
variable on the other


---
# Fit the model

* We could specify the interactions by hand by calculating new variables

--
  + Clutters up your data frame
  + Means you have to manually calculate dummy variables

--
* The formula syntax allows for easy specification of interactions

--
* Even so, we'll do it when we look at two continuous variables

---
# Centering
* Because the two terms are multiplied together (whether by you or the model)
  you can introduce high collinearity with interaction models

--
* The intercept was not meaningful before anyway (it's impossible on this
  measure to get a zero)

--

```{r center-rdg_fall}
benchmarks <- benchmarks %>%
  mutate(rdg_fall_c = scale(rdg_fall, scale = FALSE)) 
```
The above is quicker way to center, using the `scale` function, but telling it
not to do the division by the $SD$ part

--
```{r interaction-mod1}
m3b <- lm(rdg_spr ~ rdg_fall_c + ell + rdg_fall_c:ell, benchmarks)
```

---
```{r display-m3b, highlight.output = 7:8}
arm::display(m3b, detail = TRUE, digits = 3)
```

--
### How do we get the expected slope for Active ELLs? 

--
$0.447 + -0.408 = 0.039$


--
Note we also now (basically) have three intercepts. Centering effects them all!

---

```{r m3b-visualized, echo = FALSE, fig.height = 9}
benchmarks <- benchmarks %>%
  mutate(m3b_fitted = fitted(m3b))

ggplot(benchmarks, aes(rdg_fall_c, rdg_spr)) +
  geom_point(color = "gray80") +
  geom_line(aes(color = ell, y = m3b_fitted),
            size = 2) +
  labs(title = "Our fitted model",
       subtitle = "Note this looks basically the exact same as our model when
       we let ggplot2 do it")
```

---
# Thinking more about centering
What if we fit the model with the raw data?

--

```{r m3b-raw, highlight.output = c(3, 5:6)}
m3b_raw <- lm(rdg_spr ~ rdg_fall + ell + rdg_fall:ell, benchmarks)
arm::display(m3b_raw, detail = TRUE, digits = 3)
```

--
# ðŸ¤¨


---

```{r m3b_raw-visualized1, echo = FALSE, fig.height = 9}
benchmarks <- benchmarks %>%
  mutate(m3b_raw_fitted = fitted(m3b_raw))

ggplot(benchmarks, aes(rdg_fall, rdg_spr)) +
  geom_point(color = "gray80") +
  geom_line(aes(color = ell, y = m3b_raw_fitted),
            size = 2) 
```


---

```{r m3b_raw-visualized2, echo = FALSE, fig.height = 9}
pred_frame <- data.frame(rdg_fall = rep(c(0, 230), 3), 
                         ell = rep(c("Non-ELL", "Active", "Monitor"), 
                                   each = 2)) %>%
  mutate(ell = relevel(factor(ell), ref = "Non-ELL"),
         rdg_spr = predict(m3b_raw, newdata = .))

ggplot(pred_frame, aes(rdg_fall, rdg_spr)) +
  geom_line(aes(color = ell),
            size = 2) 
```

---
# Semipartial correlations

```{r semipartials-m3b}
library(lmSupport)
modelEffectSizes(m3b)
```

---
# Justified?

* Next week, we'll talk more about comparing models, and whether or not it's
"worth it" to model the interaction here.

--
* Just because the plot looks like the interaction is justified, doesn't mean 
it is.

--
* Driven partially by sample size. Does the evidence indicate it's warranted?
Not much evidence if sample size is small.

---
class: inverse center middle
# Interactions


### Two categorical IVs


---
# Move to HSB dataset

```{r hsb}
hsb <- import(here("data", "hsb2.sav"),
                  setclass = "tbl_df") %>%
  characterize() %>%
  mutate(ses = factor(ses),
         ses = relevel(ses, ref = "middle"),
         schtyp = factor(schtyp),
         schtyp = relevel(schtyp, ref = "public"))
head(hsb)
```

---
# Relation
* Let's explore the relation between school type (public/private) and SES 
category (low, middle, high) as predictors of math scores.

--
* First - plot!

```{r categorical-plot-echo, eval = FALSE}
ggplot(hsb, aes(schtyp, math, color = ses)) +
  ggbeeswarm::geom_quasirandom(dodge.width = .5) +
  geom_smooth(data = mutate(hsb, schtyp = as.numeric(schtyp)),
              method = "lm",
              aes(color = ses)) +
  theme(legend.position = "bottom")
```

---
```{r categorical-plot-eval, echo = FALSE, fig.height = 9}
ggplot(hsb, aes(schtyp, math, color = ses)) + #<<
  ggbeeswarm::geom_quasirandom(dodge.width = .5) +
  geom_smooth(data = mutate(hsb, schtyp = as.numeric(schtyp)),
              method = "lm",
              size = 2,
              aes(color = ses)) +
  theme(legend.position = "bottom")
```


---
# Fit the model

```{r m4}
m4 <- lm(math ~ schtyp + ses + schtyp:ses, hsb)
arm::display(m4, detail = TRUE, digits = 3)
```

--
### Let's interpret!

---
# Plot results

```{r m4_plot, fig.height = 5}
hsb <- hsb %>%
  mutate(fitted_m4 = fitted(m4)) 

ggplot(hsb, aes(as.numeric(schtyp) - 1, fitted_m4, color = ses)) +
  geom_line()
```

---
class: inverse center middle
# Interactions


### Two continuous IVs

---
# The glo dataset
```{r glo}
glo <- import(here("data", "glo_sim.sav"),
            setclass = "tbl_df") %>%
      characterize()
head(glo)
```

---
### Hypothesis
> The effect of adverse childhood experiences on girls sexual risk behavior is
  moderated by (i.e., depends upon) girls internalizing and externalizing
  behaviors


--
Take a minute - sketch out what this model might look like.

--
$$
risk_i = b_0 + b_1(internalizing) + b_2(externalizing) + b_3(aces) + \\\
         b_4(internalizing \times aces) + b_5(externalizing \times aces) + \\\
         e
$$

---
# Look at some plots first

* How can we visualize a continuous interaction with ggplot?

--
* We need to `cut` the continuous variable into meaningful bins

--
```{r }
glo <- glo %>%
  mutate(intern_cut = cut(internalizing, 3),
         extern_cut = cut(externalizing, 3))
glo %>%
  select(internalizing, intern_cut, externalizing, extern_cut)
```

---
```{r internalizing_int_plot}
ggplot(glo, aes(aces, sex_risk, color = intern_cut)) + 
  geom_point() +
  geom_smooth(method = "lm") +
  theme(legend.position = "top")
```

---
```{r externalizing_int_plot}
ggplot(glo, aes(aces, sex_risk, color = extern_cut)) + 
  geom_point() +
  geom_smooth(method = "lm") +
  theme(legend.position = "top")
```

---
# Fit the model

$$
risk_i = b_0 + b_1(internalizing) + b_2(externalizing) + b_3(aces) + \\\
         b_4(internalizing \times aces) + b_5(externalizing \times aces) + \\\
         e
$$

--

```{r cont_int}
m5 <- lm(sex_risk ~ internalizing + externalizing + aces + 
                    aces:externalizing + aces:internalizing, 
                    data = glo)
```

---

```{r m5-display}
arm::display(m5, detail = TRUE, digits = 3)
```

### Let's interpret!
(this gets a little trickier)

---
# APA writeup example

The model was specified with two-way interactions between each of
internalizing/externalizing behaviors and adverse childhood experiences. Both
interactions were significant, as displayed in Table 1. Overall, the effect of
adverse childhood experiences on sexual risk behavior increased as 
internalizing behaviors increased, but decreased as externalizing behaviors
increased. Specifically, a one unit increase in internalizing behaviors
corresponded to, on average, a 0.02 unit increase (95% CI: 0.01, 0.03) in the
effect of adverse childhood experiences on sexual risk scores. This interaction
uniquely accounted for 9.7% of the total variability in sexual risk scores. By
contrast, a one unit increase in externalizing behaviors corresponded to, on
average, a 0.02 unit drop in the effect of adverse childhood experiences on
sexual risk scores (95% CI: -0.02, -0.01), uniquely accounting for 7.0% of the
total variability in sexual risk scores. Figures 1 and 2 display these
interactions visually.


---
# Confidence intervals

```{r cis-m5}
confint(m5)
```

---
# Squared semipartials

```{r semipartials-m5}
modelEffectSizes(m5)
```

---
# Figure 1
(one of many methods)

```{r fig1, fig.height = 5.5}
library(visreg)
visreg(m5, xvar = "aces", by = "externalizing", overlay = TRUE)
```


---
# Figure 2
```{r fig2}
visreg(m5, xvar = "aces", by = "internalizing", overlay = TRUE)
```

---
# Take home message
* Interactions are **super** powerful, and can often lead to much deeper
  understandings of the underlying phenomena. 

--
# .center[BUT]

--
* Need to be careful - easy to overfit

--
* ALWAYS visualize your data first - use SEs in the visualization

--
* Rely heavily on theory - generally it's unwise to **test** interactions that
  don't have strong a priori support
  + Okay for exploratory work, such as visualizations, but testing is a bit of 
    a different ball game
--
* I understand today was fast - it was meant to be an intro. 
* We'll keep working with interactions all next week

---
class: inverse center middle
# Lab


