---
title: "Regression for Prediction"
author: "Daniel Anderson"
date: "Week 10"
output:
  xaringan::moon_reader:
    css: ["default", "uo", "uo-fonts", "hygge", "custom.css"]
    lib_dir: libs
    nature:
      highlightStyle: atelier-dune-light
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "https://platform.twitter.com/widgets.js"
---

```{r setup, include = FALSE}
library(here)
library(tidyverse)
library(rio)
library(ISLR)

theme_set(theme_minimal(base_size = 25))

knitr::opts_chunk$set(fig.width = 13, 
                      message = FALSE, 
                      warning = FALSE)
options(pillar.sigfig = 7)
```

# Agenda 
* Review lab from last class
* Introduce regression modeling for prediction (rather than explanation)
* Old-school methods
* Criteria for assessing model accuracy
* Bias-variance tradeoff & overfitting
  + $k$-fold cross-validation (quickly)
  
---
class: inverse middle center
background-image:url(../img/chalkboard.jpg)
background-size:cover

# What questions do you have?


---
class: inverse center middle
# Inference vs prediction

---
# What's the difference?
Focus in this class has been on inference

--
### Example RQ's
* What is the relation between the proportion of students eligible for 
  free/reduced price lunch and the school API rating?


--
* What is the effect of treatment, after controlling for participant
  demographics?


--
* To what extent does the effect of internalizing/externalizing behavior on
  sexual risk depend upon their adverse childhood experiences?

---
# Standard errors
* One of the defining features of predictive modeling is the estimation of a
  standard error for a given estimate


--
* How certain are we of the population-level estimate?


---
# Prediction
* Rather than  inferring population-level values, the focus shifts to 
  building a "machine" that produces predictions for us on new (unseen) data


--
* Less focus on population-level values, or even model interpretability.


--
* Primary focus - how well does our model perform?


---
# Why might we want to do this?
* Forecasting is useful, particularly for making individual-level decisions


--
* What is the likelihood that a kindergarten student will be reading on
  "grade-level" by third grade?


--
* What is the likelihood that a given student has a disability?


--
* What is the likelihood that a given student will drop out of high-school?


--
### In each case:
* Build a predictive model
* Use the model to forecast/predict 
* Update the forecast/prediction as more data are collected


---
# Example forecast plot
(with a more complicated model - not linear regression)

```{r forecast, echo = FALSE}
library(forecast)
USAccDeaths %>%
  tbats() %>%
  forecast() %>%
  autoplot() + 
  ggtitle("")
```


---
# A warning

![](https://media.giphy.com/media/A6H1A9rhetsXK/giphy.gif)

---
* Lots of misunderstanding out there about what predictive modeling is, and 
  many people will disparage it


--
* .bolder[My view:] 
  + these people generally have an incomplete (at best) understanding of
    predictive modeling
  + Often have antiquated views of the techniques used in predictive 
    modeling (i.e., not about maxing out $R^2$)

--
### Bottom line
Predictive modeling has been used with enormous success, and effects all of our
lives on a daily basis.

---
class: inverse center middle

# The antiquated method

---
# Stepwise regression
* Often when people bad-talk predictive modeling, it's because their thinking 
  of stepwise regression


--
### Basic approach
* Throw a bunch of variables at a problem, only keep those that are significant


---
# Approaches
* **Forward selection:** Start with no variables, add one at a time, but only
  keep if significant


--
* **Backward selection:** Start with all variables, remove one at a time based
  on the variable that is "least significant"


--
* **Stepwise selection:** Sequentially add predictor variables based on "most
  significant". After each addition, cycle through and remove an 
  non-significant variables


--
### Goal
* Find the optimal subset of variables that could be used to predict the 
  outcome

---
# Reasons for critique
* $p$ values were not designed for variable selection decisions! 

* Even if wanted to use them, why stick with 0.05, rather than what works best
for the given problem?


--
### More importantly
* People use this model **for inference** which is straight-up bonkers


--
* Tenuous even for purely exploratory purposes


--
### Why are we talking about this?
* The only reason I mention any of this is to make you aware that others may do
it, and to caution you (in the strongest way possible) not to do this.

---
# A bit of a caveat
* If used properly, it can be useful for **predictive modeling** purposes

* R will actually not even allow you (to my knowledge) to conduct stepwise
  selection using $p$ values. 

* AIC values work better, and is what R will use (information criteria )


---
# Quick illustration
### Do it with me!
(demo outside slides)

```{r forward-selection}
simple <- lm(mpg ~ 1, data = mtcars)
full <- formula(lm(mpg ~ ., mtcars)) # note call wrapped in `formula`
fwd_mod <- step(simple, 
                scope = full,
                direction = "forward")
```

---
```{r forward-anova}
arm::display(fwd_mod, detail = TRUE)
fwd_mod$anova
```


---
# Shall we change the criteria?

```{r backward-selection}
full <- lm(mpg ~ ., data = mtcars)
simple <- formula(lm(mpg ~ 1, mtcars)) # note call wrapped in `formula`
back_mod <- step(full, 
                scope = simple,
                direction = "backward")
```

---
```{r back-anova}
arm::display(back_mod, detail = TRUE)
back_mod$anova
```

---
# Change the criteria again?

```{r stepwise-selection}
simple <- lm(mpg ~ 1, data = mtcars)
full <- formula(lm(mpg ~ ., mtcars))  # note call wrapped in `formula`
step_mod <- step(simple, 
                scope = full,
                direction = "both")
```

---
```{r step-anova}
arm::display(step_mod, detail = TRUE)
step_mod$anova
```

---
# Wrapping up this section
* Applying different criteria gave us different variables


--
* From a predictive modeling framework, this isn't really a big deal - we only
care about which model performs best.


--
* From an inference perspective, it's a REALLY big deal - that's why we don't
use these models for inference 
  + Little to no theory guiding decisions, except which variables will be
    included in the automatic procedure (which is often all available variables)

---
class: inverse center middle
# How do we determine "performance"

---
class: inverse
background-image:url(../img/islr.png)
background-size:contain

# Quick plug

---
# Model performance criteria
* If the primary concern is prediction, what do we care about the most?

--

.center[### How close our predictions are, on average?]


--
.Large[ðŸ¤¨]

* Wait, isn't that basically the same thing as residual variance?


--
### No! 
* Why? We want to know how far off our model is, on average, for .bolder[cases
  outside our sample!]

---
# Test/Train datasets

* Our goal is to maximize .bolder[out-of-sample] predictive accuracy. How can 
  we get an estimate of that? Leave out part of the sample.

--

1. Split the raw data into "training" and "test" datasets


--
2. Build a model on the training dataset


--
3. Do everything you can to make the model the "best" you can


--
4. When you've settled on a .bolder[final] model, use the parameter estimates
  to predict the outcome on the test data

---
# Quick example

```{r train-test}
library(tidyverse)
set.seed(8675309)
train <- mtcars %>%
  sample_frac(.8)

test <- anti_join(mtcars, train)
nrow(train)
nrow(test)
```

---
# Fit model(s)
* Only use the training data

```{r train-mods}
m1 <- lm(mpg ~ hp, train)
m2 <- lm(mpg ~ hp + disp, train)
m3 <- lm(mpg ~ hp + disp + cyl, train)
sundry::aic_weights(m1, m2, m3)
```

--
### Settle on a model then...

---
# Predict new cases

```{r test_preds}
test <- test %>%
  mutate(pred_mpg = predict(m2, newdata = test))

test
```

---
# Calculate differences

```{r diff}
test %>%
  mutate(diff = pred_mpg - mpg)
```

--
What's the average difference? 


--
<img src="https://datainsure.com/wp-content/uploads/2018/04/ZERO-01.png"
height="175px" />

---
# What do we do?

--
* Square them


--
* Average the squared values


--
### Mean square error

.Large[
$$
MSE = \frac{1}{n}\sum(y\_i - \hat{y\_i})^2
$$
]


--
### Root mean square error

.Large[
$$
RMSE = \sqrt{\frac{1}{n}\sum(y\_i - \hat{y\_i})^2}
$$
]


---
# Evaluate 

```{r mse}
test %>%
  summarize(mse = mean((pred_mpg - mpg)^2))
```

```{r rmse}
test %>%
  summarize(rmse = sqrt(mean((pred_mpg - mpg)^2)))
```

--
* Note, you can only really do this once! Otherwise, your test dataset, in a
sense, becomes part of your training dataset.

---
## How do we develop effective models?
One approach:

--
* Split the data into multiple little samples

  
--
* Leave out one piece, try different models on the other pieces, predict on left out piece


--
* Evaluate with MSE or RMSE (or similar)
  


--
### We'll cover this more in a bit (sort of)

---
class: inverse center middle
# Overfitting
### And thinking more about functional form

.Large[
$$
\hat{Y} = f(\mathbf{X})
$$
]


---
# Predicting income
* Imagine we are trying to predict respondents' income. 
* In this case, the data have been simulated, so .bolder[we know] the 
  functional form


--

.center[

<img src="../img/true-functional-form.png" height="350px" />

]

---
# We could fit a linear relation

.center[

<img src="../img/linear-relation.png" height="450px" />

]

---
# Or a more complex model

.center[

<img src="../img/spline-good.png" height="450px" />

]

---
# Or a really complicated model

.center[

<img src="../img/spline-bad.png" height="450px" />

]

---
* The linear model doesn't fit the observed data as well .gray[(it is
  slightly underfit)]

* The really complex model model fits the observed data nearly perfectly 
  .gray[(it is overfit)]


--
* The middle model fits the observed data well, and matches the underlying
  function well. 

--
* We know this because we know the underlying function - usually we do not.

---
# Overfitting in action

.center[

![](../img/mcelreath-fig-6.3.gif)

]


---
class: inverse center middle
# Bias-Variance Trade-off

---
# Definitions
* Bias: How well does the model represent the observed data?


--
* Variance: How much does the model change from sample to sample?


--
### Linear models...
Tend to have low variance

--
### But also...
Can have high bias, if the underlying functional form is not *truly* linear



---
# Highly flexible models
* Tend to have low bias (representing the observed data well), but can
  easily overfit, leading to high variance


--
### The goal
* Balance the bias-variance trade-off

--
* Note, this is an incredibly useful perspective **even if your goal is
  to model for inference.**


---
# More gifs!
Why the linear model has low variance

![](../img/linear.gif)

---
# A crazy polynomial model
Why more complicated models have higher variance
![](../img/cray-poly.gif)


---
# k-fold cross-validation 
### Quickly 

![](../img/k-fold-cv.png)


---
```{r k-fold-cv}
library(modelr)
iris %>%
  crossv_kfold(10) %>%
  mutate(model = map(train, ~lm(Sepal.Length ~ Petal.Length, data=.)))
```

---
```{r k-fold-cv-rmse}
iris %>%
  crossv_kfold(10) %>%
  mutate(model = map(train, ~lm(Sepal.Length ~ Petal.Length, data=.)),
         rmse = map2_dbl(model, test, rmse)) #<<
```

---
```{r k-fold-cv-rmse-summary}
iris %>%
  crossv_kfold(10) %>%
  mutate(model = map(train, 
                     ~lm(Sepal.Length ~ Petal.Length, 
                         data=.)),
         rmse = map2_dbl(model, test, rmse)) %>%
  summarize(mean_rmse = mean(rmse)) #<<
```

---
# What if we model it cubicly ðŸ¤·

```{r k-fold-cv-rmse-nonlinear}
iris %>%
  crossv_kfold(10) %>%
  mutate(model = map(train, 
                     ~lm(Sepal.Length ~ poly(Petal.Length, 3), 
                         data=.)),
         rmse = map2_dbl(model, test, rmse)) %>%
  summarize(mean_rmse = mean(rmse)) #<<
```

---
class: inverse middle center
background-image:url(../img/kaggle.png)
background-size:contain


---
# Want some practice?
[kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
* Competitive data science platform  


--
* Ways for companies to crowdsource problems


--
* Can compete individually or in teams


--
* Great way to learn!

---
class: inverse center middle
# Demo
Let's try a model together!


